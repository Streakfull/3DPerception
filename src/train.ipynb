{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import mkdir,seed_all\n",
    "from omegaconf import OmegaConf\n",
    "from cprint import *\n",
    "from datasets.shape_net.shape_net_vox import ShapeNetVox\n",
    "from models.dummy_classifier import DummyClassifier\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.visualizations import save_voxels\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Expirement Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_path = \"./configs/global_configs.yaml\"\n",
    "global_configs = OmegaConf.load(configs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m{'name': 'dummy_classification', 'description': 'Initial overfitting Experiment on shape net', 'extra_notes': 'none', 'experiment_id': 3, 'logs_dir': 'logs', 'is_train': True, 'device': 'cuda:0', 'batch_size': 8, 'n_epochs': 20, 'append_loss_every': 1, 'print_every': 1, 'validate_every': 5, 'save_every': 5, 'save_every_nepochs': 10, 'start_epoch': 0, 'start_iteration': 0, 'visualize_every': 5, 'load_ckpt': False, 'ckpt_path': 'test_path'}\u001b[0m\n",
      "\u001b[94m- logs directory found\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93m- Creating new directory logs/dummy_classification/3\u001b[0m\n",
      "\u001b[93m- Creating new directory logs/dummy_classification/3/checkpoints\u001b[0m\n",
      "\u001b[93m- Creating new directory logs/dummy_classification/3/tb\u001b[0m\n",
      "\u001b[93m- Creating new directory logs/dummy_classification/3/visuals\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "seed_all(111)\n",
    "training_config = global_configs[\"training\"]\n",
    "today = time.strftime(\"%Y-%m-%d\")\n",
    "cprint.ok(training_config)\n",
    "description = training_config[\"description\"]  # Describe Experiment params here\n",
    "logs_dir = training_config[\"logs_dir\"]\n",
    "mkdir(logs_dir)\n",
    "experiment_dir = f\"{logs_dir}/{training_config['name']}/{training_config['experiment_id']}\"\n",
    "mkdir(experiment_dir)\n",
    "loss_log_title = \"Loss Log \" + today\n",
    "\n",
    "with open(f\"{experiment_dir}/description.txt\", \"w\") as file1:\n",
    "    file1.write(description)\n",
    "\n",
    "with open(f\"{experiment_dir}/global_configs.txt\", \"w\") as file1:\n",
    "    file1.write(str(training_config))\n",
    "\n",
    "with open(f\"{experiment_dir}/loss_log.txt\", \"w\") as file1:\n",
    "    file1.write(loss_log_title)\n",
    "    file1.write(\"\\n\")\n",
    "\n",
    "\n",
    "mkdir(f\"{experiment_dir}/checkpoints\")\n",
    "mkdir(f\"{experiment_dir}/tb\")\n",
    "mkdir(f\"{experiment_dir}/visuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset & Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  2\n"
     ]
    }
   ],
   "source": [
    "# Change overfit param here & cat here\n",
    "global_dataset_config = global_configs[\"dataset\"]\n",
    "local_dataset_config = global_dataset_config[\"shape_net_vox\"]\n",
    "dataset = ShapeNetVox(global_dataset_config, local_dataset_config)\n",
    "print('length: ', len(dataset))\n",
    "dataset[0]\n",
    "# train_ds, valid_ds, test_ds = torch.utils.data.random_split(\n",
    "#     dataset, [1,1,1])\n",
    "\n",
    "train_ds,valid_ds, test_ds = dataset, dataset, dataset\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_ds,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "    batch_size=training_config['batch_size'],   # The size of batches is defined here\n",
    "    shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "    num_workers=6,   # Data is usually loaded in parallel by num_workers\n",
    "    pin_memory=True,  # This is an implementation detail to speed up data uploading to the GPU\n",
    "    # worker_init_fn=train_dataset.worker_init_fn  TODO: Uncomment this line if you are using shapenet_zip on Google Colab\n",
    ")\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_ds,   # Datasets return data one sample at a time; Dataloaders use them and aggregate samples into batches\n",
    "    batch_size=training_config['batch_size'],   # The size of batches is defined here\n",
    "    shuffle=True,    # Shuffling the order of samples is useful during training to prevent that the network learns to depend on the order of the input data\n",
    "    num_workers=6,   # Data is usually loaded in parallel by num_workers\n",
    "    pin_memory=True,  # This is an implementation detail to speed up data uploading to the GPU\n",
    "    # worker_init_fn=train_dataset.worker_init_fn  TODO: Uncomment this line if you are using shapenet_zip on Google Colab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mUsing CPU\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_configs = global_configs[\"model\"][\"dummy_classifier\"]\n",
    "model = DummyClassifier(model_configs)\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available() and training_config['device'].startswith('cuda'):\n",
    "    device = torch.device(training_config['device'])\n",
    "    cprint.ok('Using device:', training_config['device'])\n",
    "else:\n",
    "    cprint.warn('Using CPU')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "if(training_config[\"load_ckpt\"]):\n",
    "    model.load_ckpt(training_config['ckpt_path'])\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    torch.cuda.mem_get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_running = 0.\n",
    "best_loss_val = np.inf\n",
    "model.train()\n",
    "start_iteration = training_config[\"start_iteration\"]\n",
    "tb_dir = f\"{experiment_dir}/tb\"\n",
    "writer = SummaryWriter(log_dir=tb_dir)\n",
    "model_checkpoint_path = f\"{experiment_dir}/checkpoints\"\n",
    "loss_log_name = f\"{experiment_dir}/loss_log.txt\"\n",
    "visuals_path = f\"{experiment_dir}/visuals\"\n",
    "last_loss = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, writer):\n",
    "    global best_loss_val\n",
    "    global last_loss\n",
    "    global start_iteration\n",
    "    train_loss_running = 0.\n",
    "    iteration_count = 0\n",
    "    for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n",
    "         iteration = epoch * len(train_dataloader) + batch_idx \n",
    "         if(iteration<= start_iteration):\n",
    "            continue\n",
    "         ShapeNetVox.move_batch_to_device(batch, device)\n",
    "         model.step(batch)\n",
    "         metrics = model.get_metrics()\n",
    "         loss = metrics[\"loss\"]\n",
    "         train_loss_running += loss\n",
    "         iteration_count += 1\n",
    "\n",
    "         if iteration % training_config[\"append_loss_every\"] == (training_config[\"append_loss_every\"] - 1) or (epoch==0 and iteration==0):\n",
    "            message = '(epoch: %d, iters: %d, loss: %.6f)' % (epoch, iteration, loss.item())\n",
    "            with open(loss_log_name, \"a\") as log_file:\n",
    "                log_file.write('%s\\n' % message)\n",
    "            print(loss)\n",
    "\n",
    "         if iteration % training_config[\"visualize_every\"] == (training_config[\"visualize_every\"] - 1):\n",
    "            # Do visualizations here\n",
    "            cprint.ok(\"visuals here\")\n",
    "        \n",
    "         if iteration % training_config['print_every'] == (training_config['print_every'] - 1) or (epoch==0 and iteration==0):\n",
    "            avg_train_loss = train_loss_running / iteration_count\n",
    "            cprint.warn(f'[{epoch:03d}/{batch_idx:05d}] train_loss: {avg_train_loss:.6f}')\n",
    "            writer.add_scalar(\"Train/Loss\", avg_train_loss, iteration)\n",
    "            last_loss = avg_train_loss\n",
    "            train_loss_running = 0.\n",
    "            iteration_count = 0\n",
    "        \n",
    "         if iteration % training_config['save_every'] == (training_config['save_every'] - 1):\n",
    "            model.save(model_checkpoint_path, \"latest\")\n",
    "\n",
    "         if iteration % training_config['validate_every'] == (training_config['validate_every'] - 1) or (epoch == 0 and iteration == 0):\n",
    "            cprint.ok(\"Running Validation\")\n",
    "            model.eval()\n",
    "            loss_val = 0.\n",
    "            index_batch = 0\n",
    "            for batch_idx, batch_val in tqdm(enumerate(validation_dataloader)):\n",
    "                ShapeNetVox.move_batch_to_device(batch_val, device)\n",
    "                with torch.no_grad():\n",
    "                    model.inference(batch_val)\n",
    "                    metrics = model.get_metrics()\n",
    "                    loss_val += metrics[\"loss\"]\n",
    "                    index_batch += 1\n",
    "            avg_loss_val = loss_val / (index_batch)\n",
    "\n",
    "            #Do visualizations here\n",
    "            if avg_loss_val < best_loss_val:\n",
    "                model.save(model_checkpoint_path, \"best\")\n",
    "                best_loss_val = avg_loss_val\n",
    "            \n",
    "            cprint.warn(f'[{epoch:03d}/{batch_idx:05d}] val_loss: {avg_loss_val:.6f} | best_loss_val: {best_loss_val:.6f}')\n",
    "            writer.add_scalar(\"Validation/Loss\", avg_loss_val, iteration)\n",
    "            writer.add_scalars('Validation/LossComparison',\n",
    "                   { 'Training' : last_loss, 'Validation' : avg_loss_val },\n",
    "                    iteration)\n",
    "            writer.flush()\n",
    "         return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117fb6941aef4cfeb19758467e1192d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7090f2bc7a44778a43fbb7e1a89080d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/dummy_classification/3/checkpoints/epoch-0.ckpt created\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344a7818318a43d5bc1326888677182e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x00000171CA3AB910>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python310\\lib\\multiprocessing\\connection.py\", line 137, in __del__\n",
      "    self._close()\n",
      "  File \"c:\\Python310\\lib\\multiprocessing\\connection.py\", line 282, in _close\n",
      "    _CloseHandle(self._handle)\n",
      "OSError: [WinError 6] The handle is invalid\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'iou' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m start_epoch:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# if(epoch % config[\"save_every_nepochs\"]==0):\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_checkpoint_path, epoch)\n",
      "Cell \u001b[1;32mIn [7], line 21\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch, writer)\u001b[0m\n\u001b[0;32m     18\u001b[0m iteration_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m training_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend_loss_every\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m (training_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend_loss_every\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (epoch\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m iteration\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m    message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(epoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, iters: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[38;5;124m, iou: \u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, iteration, loss\u001b[38;5;241m.\u001b[39mitem(), \u001b[43miou\u001b[49m\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     22\u001b[0m    \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loss_log_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m log_file:\n\u001b[0;32m     23\u001b[0m        log_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m message)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'iou' is not defined"
     ]
    }
   ],
   "source": [
    "start_epoch = training_config[\"start_epoch\"]\n",
    "for epoch in tqdm(range(training_config['n_epochs'])):\n",
    "    if epoch < start_epoch:\n",
    "        continue\n",
    "    avg_loss = train_one_epoch(epoch, writer)\n",
    "    # if(epoch % config[\"save_every_nepochs\"]==0):\n",
    "    model.save(model_checkpoint_path, epoch)\n",
    "    model.update_lr()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
